# 컨볼루션 신경망 (CNN)

## 1. 완전 연결 신경망(Fully-Connected Neural Network)의 한계와 CNN의 등장 배경

### 1.1 완전 연결 신경망의 한계

- **Fully-Connected (완전 연결) 특성**:
  - 이론적으로는 학습을 통해 어떤 문제든 해결 가능성을 가짐
  - **1차원 형태의 데이터 입력**을 요구함 → 이미지와 같은 다차원 데이터를 처리하기 위해서는 **Flatten 과정**이 필요

- **Flattening의 문제점**:
  - **공간적 정보(Locality) 손실**: 3차원 이미지를 1차원으로 변환하면 픽셀 간 인접 관계가 깨짐
  - **독립성 가정 위배**: 원래 데이터의 구조적 특성이 사라져 각 입력 요소가 독립적인 것처럼 처리됨
  - **차원의 저주(Curse of Dimensionality)**: 고차원 데이터를 1차원으로 펼치면서 입력 차원이 급증
  - 이러한 문제로 인해 **모델의 범용성(Generalization)이 저하**됨

### 1.2 과거의 해결 노력

- **강력한 전처리(Preprocessing)**:
  - 이미지 데이터를 모델이 잘 인식하도록 정중앙에 위치시키기
  - 크기를 일정하게 맞추기
  - 회전, 밝기 등 표준화

- **피처 엔지니어링(Feature Engineering)**:
  - 입력 데이터에서 중요 특징(Feature)을 수동으로 추출/가공
  - 어떤 특징을 어떻게 다뤄야 성능이 좋을지에 대한 연구에 많은 시간과 노력 투자
  - 도메인 지식에 크게 의존

- **데이터 증강(Data Augmentation)**:
  - 데이터가 중앙에 있지 않거나 다양한 변형이 있어도 모델이 강인하게 학습하도록 지원
  - 근본적인 해결책은 아니었음

## 2. 컨볼루션 신경망(CNN)의 핵심 아이디어

### 2.1 Invariance와 Equivariance

- **Invariance (불변성)**:
  - 입력의 위치가 변해도 출력은 변하지 않는 성질
  - 예: "Welcome"이라는 단어가 음성 신호의 어느 부분에 있든 "Welcome이 있다"는 동일한 출력
  - **Shift Invariance (이동 불변성)**: 패턴의 위치 변화에 영향받지 않는 특성

- **Equivariance (등변성/동변성)**:
  - 입력의 위치가 변하면 출력의 위치도 그에 맞춰 동일하게 변하는 성질
  - 예: 이미지에서 고양이 위치가 오른쪽으로 이동하면, 특징 맵에서도 고양이 활성화 영역이 오른쪽으로 이동
  - **컨볼루션 연산은 본질적으로 Equivariant** - 필터가 이미지 전체를 순회하며 특징을 감지
  - CNN에서 **풀링(Pooling) 레이어**를 통해 부분적인 Invariance 획득

### 2.2 CNN의 "스캔(Scan)" 방식

- **특징 감지 메커니즘**:
  - 작은 윈도우(필터)를 이동시키면서 각 영역에 특정 패턴이 있는지 확인
  - 음성 신호에서 "Welcome" 단어 찾기: 작은 "Welcome-탐지기" 필터를 이동하며 감지
  - 이미지에서 "꽃" 찾기: 이미지 각 부분을 작은 필터로 스캔하며 "꽃" 특징 감지

- **특징 추출 과정**:
  - 입력 전체를 스캔하여 "있다/없다" 형태의 특징 맵(Feature Map) 생성
  - 이 특징 맵을 학습에 활용하여 모델의 유연성과 효율성 향상

## 3. 컨볼루션(Convolution) 연산의 이해

### 3.1 기본 개념

- **컨볼루션 연산**:
  - 원래 **신호 처리(Signal Processing)** 분야에서 사용되던 기법
  - 수학적으로 두 함수 중 하나를 **뒤집고(flip)** 이동하면서 중첩 영역의 내적(또는 적분) 계산
  - `scipy.signal.convolve(a, b)`: 두 신호 `a`와 `b` 사이의 컨볼루션 계산

- **상관관계(Correlation)와 컨볼루션(Convolution)의 차이**:
  - **상관관계 연산**: 커널을 뒤집지 않고 내적 연산 수행
  - **컨볼루션 연산**: 커널을 상하좌우로 뒤집은 후 상관관계 연산 수행
  - **주요 차이점**: 커널을 뒤집느냐의 여부

### 3.2 컨볼루션 연산의 특성

- **왜 컨볼루션을 사용하는가?**:
  - **교환 법칙(Commutative Property) 성립**: $A * B = B * A$ (여기서 $*$는 컨볼루션 연산)
  - 교환 법칙이 성립하면 이론 정립과 수학적 처리가 용이
  - **딥러닝에서의 구현**: 대부분의 딥러닝 라이브러리는 실제로는 **상관관계 연산을 "컨볼루션"이라고 명명**
  - 커널 가중치가 학습을 통해 최적화되므로 초기에 뒤집혀 있든 아니든 최종 결과에 큰 영향 없음

- **컨볼루션 연산의 의미**:
  - **내적 연산의 공간적 의미**: 주변 픽셀들의 속성을 종합하여 하나의 값으로 표현
  - 입력 데이터에서 **특정 패턴(특징)의 존재 여부를 감지**하는 역할

### 3.3 Zero Padding과 Stride

- **Zero Padding (제로 패딩)**:
  - 입력 데이터 가장자리에 0 값을 추가하는 기법
  - **목적**:
    - 출력 특징 맵의 크기를 입력과 동일하게 유지/조절
    - 입력 데이터 가장자리 픽셀들도 동등하게 참여 (정보 손실 방지)

- **Stride (스트라이드)**:
  - 컨볼루션 필터를 이동시키는 간격
  - Stride가 1이면 한 픽셀씩, 2이면 두 픽셀씩 이동
  - Stride 값이 클수록 출력 특징 맵의 크기는 작아짐 (다운샘플링 효과)

## 4. CNN의 구조적 특징과 장점

### 4.1 역사적 배경 및 구조적 특징

- **얀 르쿤(Yann LeCun)의 기여**:
  - 1989년 LeNet-1을 통해 컨볼루션 연산을 신경망에 효과적으로 결합
  - 현대 CNN 아키텍처의 기초를 확립

- **컨볼루션 연산의 선형성**:
  - $Wx+b$ 형태의 연산으로 기존 신경망 구조에 자연스럽게 통합 가능

- **실제 구현 시 병렬 처리**:
  - 이론적으로는 필터가 순차적으로 이동하며 연산
  - 실제 GPU 구현에서는 각 위치 연산을 독립적으로 분리하여 병렬 처리 (벡터화)
  - 이를 통해 계산 속도를 크게 향상

### 4.2 주요 개념 및 장점

- **Locally Connected & Shared Weights (지역적 연결 및 가중치 공유)**:
  - **지역적 연결**: 각 뉴런이 입력의 일부 지역(Receptive Field, 수용 영역)에만 연결
  - **가중치 공유**: 동일한 필터(커널)가 입력 전체를 순회하며 사용됨
  - **장점**:
    - 파라미터 수 대폭 감소 → 모델 복잡도 감소 및 과적합 방지
    - 학습 효율 향상
    - 위치 변화에도 동일 특징 감지 가능 (Shift Invariance에 기여)

- **CNN의 기본 가정**:
  - **Stationarity of Statistics (통계적 항상성)**: 이미지 내 서로 다른 부분의 통계적 특성이 유사
  - **Locality of Pixel Dependencies (픽셀 종속성의 지역성)**: 인접 픽셀 간 높은 연관성, 원거리 픽셀 간 낮은 연관성

- **Statistical Efficiency (통계적 효율성)**:
  - 가중치 공유와 지역적 연결을 통해 적은 파라미터로 효과적인 특징 추출
  - 동일 양의 학습 데이터로 더 많은 정보 학습 → 일반화 능력 향상

- **공간적 정보 보존**:
  - Flatten 없이 2D/3D 형태 그대로 처리
  - 입력의 공간적 구조를 유지하며 특징 학습

## 5. 풀링 (Pooling) 레이어

### 5.1 개념 및 목적

- **목적**:
  - 특징 맵의 **차원 축소**(Downsampling) → 계산량 감소 및 과적합 방지
  - 입력의 작은 변화(이동, 회전, 크기 변화 등)에 대한 **불변성(Invariance) 제공**
  - 수용 영역(Receptive Field) 확장 → 더 넓은 컨텍스트 고려

- **종류**:
  - **Max Pooling (최대 풀링)**: 특정 영역에서 최대값 선택 (가장 두드러진 특징 강조)
  - **Average Pooling (평균 풀링)**: 특정 영역 값들의 평균 사용

### 5.2 풀링에 대한 논쟁과 대안

- **풀링 레이어의 역할에 대한 논쟁**:
  - **얀 르쿤**: Max Pooling이 불변성을 보존하는 장점 강조
  - **제프리 힌튼**: Max Pooling은 잘못된 접근이며, "Invariance"보다 "Equivariance"가 중요하다고 주장
    (이러한 관점은 후에 캡슐 네트워크 개발로 이어짐)

- **풀링 없는 CNN (All Convolutional Net)**:
  - 최근 경향: 풀링 레이어 대신 Stride가 큰 컨볼루션 레이어로 차원 축소
  - VAE, GAN 등에서 자주 사용되는 접근법

## 6. CNN 아키텍처 설계 시 고려 사항

- **컨볼루션 필터 크기 (Kernel Size)**:
  - 일반적으로 3x3, 5x5, 7x7 등 사용
  - 작은 커널 여러 개를 쌓는 것이 큰 커널 하나보다 효율적인 경우가 많음
  - 특징의 크기와 복잡도에 맞게 선택

- **풀링 레이어 사용 여부 및 종류**:
  - Max Pooling: 두드러진 특징 보존
  - Average Pooling: 전반적인 특성 보존
  - 생략 여부: 일부 모델은 Strided Convolution으로 대체

- **스트라이드 (Stride)**:
  - 필터 이동 간격 설정
  - 큰 스트라이드: 차원 축소 효과
  - 작은 스트라이드: 세밀한 특징 포착

- **패딩 (Padding)**:
  - Valid Padding: 패딩 없음 → 출력 크기 감소
  - Same Padding: Zero Padding 사용 → 출력 크기 유지
  - 출력 크기 관계: 
    - 출력 크기 = (입력 크기 - 커널 크기 + 2*패딩) / 스트라이드 + 1

## 7. CNN의 계층적 특징 학습

### 7.1 계층적 특징 추출

- **컨볼루션 레이어의 중첩**:
  - CNN은 여러 컨볼루션 레이어를 순차적으로 쌓아 구성

- **계층별 특징 학습**:
  - **초기 레이어(얕은 층)**: 단순한 특징 감지 (선, 모서리, 색상 등)
  - **중간 레이어**: 질감, 패턴, 단순한 형태 감지
  - **후기 레이어(깊은 층)**: 복잡하고 추상적인 특징 감지 (객체의 부분, 객체 전체 등)
  - 이러한 계층적 특징 학습을 통해 복잡한 시각적 패턴을 효과적으로 인식

### 7.2 Global Average Pooling (GAP)

- **개념 및 용도**:
  - CNN의 마지막 컨볼루션 레이어 다음에 주로 사용
  - 각 특징 맵의 모든 픽셀 값의 평균을 계산하여 하나의 값으로 변환

- **장점**:
  - 완전 연결 계층(FC Layer) 대체 → 파라미터 수 감소 및 과적합 방지
  - **Regularizer 역할** 수행
  - 입력 이미지의 공간적 변환에 대해 더 강인한(Robust) 특성
  - 내부 동작 해석이 FC 레이어보다 용이

## 8. CNN의 한계 및 발전 방향

### 8.1 기존 CNN의 한계

- **Hinton의 CNN 비판**: "What is wrong with convolutional neural nets?"
  - 객체의 상대적 공간 관계나 자세 변화 등을 제대로 처리하지 못하는 한계
  - 공간적 계층 구조(Spatial Hierarchy) 모델링의 한계
  - 회전, 크기 등 다양한 변환에 대한 진정한 불변성 부재

### 8.2 발전 방향 및 연구 동향

- **캡슐 네트워크(Capsule Network)**:
  - 객체의 부분들 간의 위치 관계를 보존하는 새로운 접근법
  - Dynamic Routing 메커니즘을 통한 특징 통합

- **어텐션 메커니즘(Attention Mechanism)**:
  - 이미지의 중요 부분에 집중하는 메커니즘 도입
  - Transformer 기반 컴퓨터 비전 모델로 확장 (Vision Transformer 등)

- **CNN 시각화 및 해석**:
  - 모델의 각 레이어가 학습하는 특징 시각화 기법 연구
  - Grad-CAM, Feature Visualization 등의 방법론
  - 모델의 내부 동작 이해 및 개선에 활용

## 9. CNN 모델의 발전사 및 주요 아키텍처

### 9.1 MLP에서 CNN으로의 발전: 왜 컨볼루션 네트워크인가?

- **MLP (Fully Connected Layer)의 한계**:
  - 이미지와 같은 고차원 데이터(예: 224x224x3 이미지)를 처리할 때, 입력층의 모든 픽셀을 1차원으로 펼쳐서 은닉층의 모든 뉴런과 연결하면 **파라미터 수가 기하급수적으로 증가**
  - 엄청난 계산량과 메모리 요구량, 그리고 과적합(Overfitting) 문제 야기
  - 이미지의 **공간적 구조(Spatial Structure)** 정보를 활용하지 못함

- **CNN의 등장 배경**:
  - 이미지의 지역적 특징(Local Features)을 효과적으로 추출하고, 파라미터 수를 줄이기 위해 고안
  - 인간의 시각 처리 방식(시야의 특정 부분에 집중하여 특징을 인식하고 이를 조합)에서 영감

### 9.2 CNN의 일반적인 구조: 특징 추출과 분류

- **구조적 구분**:
  1. **특징 추출 부분 (Feature Extraction Part)**:
     - 여러 개의 컨볼루션 레이어와 풀링 레이어로 구성
     - 입력 이미지로부터 계층적인 특징을 추출
  
  2. **분류/예측 부분 (Classification/Prediction Part)**:
     - 주로 **완전 연결 계층 (Fully Connected Layer)**으로 구성
     - 특징 추출 부분에서 나온 고수준의 특징 맵을 1차원으로 펼친 후(Flatten), 최종 분류/예측 수행

- **리프리젠테이션 러닝 (Representation Learning)**:
  - 컨볼루션 레이어는 입력 이미지를 **특징이 있는지 없는지를 나타내는 데이터(특징 맵)**로 변환
  - 원본 이미지보다 더 유용하고 추상적인 **표현(Representation)**을 학습
  - **XAI (Explainable AI)**: 이러한 블랙박스 모델의 의사결정 과정을 이해하고 설명하려는 연구 분야

- **모델 결합 관점**: 
  - CNN은 특징 추출 모델과 예측 모델(MLP) 두 개가 결합된 형태로 볼 수 있음
  - 여러 모델을 결합하여 End-to-End로 학습시키는 성공적 사례

### 9.3 LeNet (1989-1998)

- **LeNet-5 (1998)**: 얀 르쿤이 개발한 현대 CNN의 원형
  - **논문**: "Gradient-Based Learning Applied to Document Recognition"
  - **구조**: 입력(32x32) → Conv → Subsampling(Pooling) → Conv → Subsampling(Pooling) → FC → FC → Output(Softmax)
  - **응용**: MNIST 손글씨 숫자 인식 문제에 높은 성능 달성
  - **주요 아이디어**: 컨볼루션, 풀링, 완전 연결 계층의 조합
  - **한계**: 작은 데이터셋과 제한된 컴퓨팅 파워로 인해 더 깊고 복잡한 모델로 발전하지 못함

### 9.4 AlexNet (2012)

- **LSVRC-2012 우승**: 딥러닝 부활을 알린 획기적 모델
  - 이전 모델들보다 크게 낮은 오류율(Top-5 error 15.3%) 기록
  - 규모가 크게 확장된 컨볼루션 5개, FC 3개의 구조

- **주요 혁신 및 특징**:
  - **ReLU 활성화 함수 사용**: Sigmoid/Tanh 대신 ReLU로 학습 속도 향상 및 그래디언트 소실 완화
  - **Multiple GPUs 활용**: GTX 580 GPU 2개를 병렬로 사용
  - **Local Response Normalization (LRN)**: 측면 억제 효과 모방
  - **Overlapping Pooling**: 풀링 윈도우가 겹치도록 하여 정보 손실 감소
  - **데이터 증강 (Data Augmentation)**: 이미지 변환을 통해 학습 데이터 확장
  - **드롭아웃 (Dropout)**: FC Layer에서 과적합 방지

- **학습 시간**: 약 5~6일 소요 (GPU 2개 사용)

### 9.5 ZFNet (2013)

- **LSVRC-2013 우승**: AlexNet 구조 개선 및 CNN 내부 동작 시각화

- **주요 기여**:
  - **Deconvolutional Network (역합성곱 신경망)**: 각 레이어가 학습하는 특징 시각화
    - 첫 번째 레이어: 선, 면, 색상 등 단순 패턴 감지
    - 후속 레이어: 더 복잡한 패턴 감지
  - AlexNet의 하이퍼파라미터(필터 크기, 스트라이드 등) 조정으로 성능 개선

### 9.6 VGGNet (2014)

- **LSVRC-2014 준우승**: 깊은 네트워크 구조의 가능성 증명

- **주요 특징**:
  - **작은 크기(3x3)의 컨볼루션 필터 일관적 사용**: 
    - 여러 3x3 필터 중첩 = 더 큰 필터와 동일 Receptive Field + 파라미터 수 감소 + 비선형성 증가
  - **깊이의 중요성**: 레이어 깊이와 성능 향상 간의 상관관계 실험적 증명
  - **단순하고 균일한 구조**: 이해하기 쉽고 변형하기 용이함
  - FC Layer는 LeNet과 유사하게 3개 사용
  - LRN 미사용 (성능에 큰 영향 없음)

- **영향**: Network in Network(NIN)의 아이디어에 영향 받음
- **학습 시간**: GPU 4개로 2~3주 소요

### 9.7 Network in Network (NIN) (2013)

- **LSVRC 순위권 밖이지만 현대 CNN 구조에 큰 영향을 미친 논문**

- **주요 아이디어**:
  - **MLPConv Layer (Micro Network)**: 선형 컨볼루션 필터 대신 작은 MLP를 필터로 사용
  - **1x1 Convolution**: 
    - 채널 간 연산을 통한 차원 축소/확장 및 비선형성 추가
    - FC Layer와 유사한 기능을 수행하면서 파라미터 수 감소
  - **Global Average Pooling (GAP)**:
    - Flatten + FC Layer 대신 각 특징 맵의 평균값 사용
    - 파라미터 수 대폭 감소 및 과적합 방지
    - 입력 이미지 크기에 유연하게 대응

### 9.8 GoogLeNet (Inception) (2014)

- **LSVRC-2014 우승**: "Going Deeper with Convolutions" 논문

- **주요 특징 - Inception Module**:
  - 다양한 크기의 컨볼루션 필터(1x1, 3x3, 5x5)와 풀링을 병렬 수행
  - 결과를 채널 방향으로 합침(Concatenate)
  - 네트워크가 다양한 스케일의 특징을 학습하고 최적 필터 크기 선택
  
  - **1x1 Convolution 활용**: 
    - 3x3, 5x5 컨볼루션 전에 사용하여 입력 채널 수 감소(Bottleneck)
    - 계산량 크게 감소 및 모델 효율성 향상
  
  - **보조 분류기 (Auxiliary Classifiers)**: 
    - 중간에 보조 분류기를 두어 그래디언트 소실 완화
    - 테스트 시에는 제거

  - **Global Average Pooling 사용**: FC Layer의 파라미터 수 감소

- **깊이**: 22개 레이어의 매우 깊은 구조

### 9.9 ResNet (2015)

- **LSVRC-2015 우승**: **인간 분류 성능 초월 (Top-5 error 3.57%)**

- **주요 혁신 - Residual Block (잔차 블록)**:
  - 네트워크 깊이 증가에 따른 성능 저하 문제(Degradation Problem) 해결
  - **Skip Connection (Identity Mapping)** 개념 도입
  - 입력 $x$를 몇 개 레이어를 거친 출력 $F(x)$에 더하여 최종 출력 $H(x) = F(x) + x$ 생성
  - 잔차($F(x)$)를 학습하여 불필요 레이어는 $F(x) \approx 0$으로 설정 가능
  - 매우 깊은 네트워크(100층 이상, 1000층까지)도 효과적으로 학습

- **VGGNet의 영향**: 기본 컨볼루션 레이어는 VGGNet의 3x3 필터 구조 계승
- **풀링 레이어 사용 최소화**: 스트라이드(Stride)가 2인 컨볼루션으로 대체

## 10. CNN 관련 주요 기술 및 동향

### 10.1 풀링 (Pooling)

- **목적**:
  - 특징 맵 크기 축소로 계산량 감소
  - 위치 변화에 강인한(Robust) 특징 학습 지원

- **종류**:
  - **Max Pooling**: 영역 내 최대값 선택 (두드러진 특징 강조)
  - **Average Pooling**: 영역 내 평균값 사용 (전반적 특성 보존)

- **최근 동향**: 
  - 스트라이드 컨볼루션으로 대체 추세
  - 정보 손실 문제로 사용 빈도 감소

### 10.2 정규화 (Normalization)

- **LRN (Local Response Normalization)**:
  - AlexNet에서 사용되었으나 효과 미미
  - 현재는 거의 사용되지 않음

- **배치 정규화 (Batch Normalization)**:
  - 현대 CNN 및 딥러닝 모델에서 필수적인 기법
  - 학습 안정화, 그래디언트 소실 완화, 학습 속도 향상 효과
  - 드롭아웃 필요성 감소
  - GoogLeNet 이후 보편화

### 10.3 드롭아웃 (Dropout)

- **목적**: FC Layer의 과적합 방지
- **원리**: 학습 시 일부 뉴런 무작위 비활성화 → 앙상블 효과
- **최근 동향**: 
  - 배치 정규화 등장으로 중요도 감소
  - 복잡한 모델에서는 오히려 수렴을 방해할 수 있음

### 10.4 앙상블 (Ensemble)

- **개념**: 여러 모델의 예측을 결합하여 단일 모델보다 더 좋은 성능 도출
- **활용**: 
  - 대회 등에서 성능을 극한으로 끌어올릴 때 사용
  - 실제 서비스에서는 구현 복잡성으로 적용 난이도 높음
- **관련성**: 드롭아웃도 일종의 앙상블 효과 구현

### 10.5 모델 경량화 (Model Compression/Quantization)

- **배경**: 
  - ResNet 이후 인간 수준 성능 달성
  - 리소스 제한적 환경(모바일, 엣지 디바이스)에서 구동 필요성 증대

- **주요 방법**:
  - **가지치기(Pruning)**: 중요도 낮은 가중치/뉴런 제거
  - **양자화(Quantization)**: 32비트 부동소수점 → 16/8비트 정수형 변환
  - **지식 증류(Knowledge Distillation)**: 큰 모델(교사)의 지식을 작은 모델(학생)로 전달
  - **아키텍처 설계**: 경량 아키텍처 직접 설계 (MobileNet, EfficientNet 등)

### 10.6 불변성(Invariance)과 등변성(Equivariance)

- **Translation Invariance(평행 이동 불변성)**:
  - 기본적인 CNN의 강점
  - 컨볼루션 + 풀링을 통해 구현

- **한계**:
  - 회전(Rotation), 크기(Scale), 시점(Perspective) 변화에 취약
  - 이를 해결하기 위한 변형 컨볼루션 연구 활발
    - Deformable Convolution
    - Capsule Network
    - Spatial Transformer Network 등

### 10.7 연구 동향

- **성능과 효율성의 균형**:
  - 극한의 성능 추구 방향과 실용적 경량화/효율화 방향으로 이원화

- **XAI (Explainable AI)**:
  - 모델 내부 동작 이해 및 설명 가능성 연구
  - Grad-CAM, LIME, SHAP 등 다양한 기법 개발

- **기술 융합**:
  - 자기지도학습(Self-supervised Learning)을 통한 라벨 효율성 향상
  - 트랜스포머(Transformer)와 CNN의 결합 (Vision Transformer)
  - 메타러닝(Meta-learning)을 통한 빠른 적응 능력 습득

## 핵심 요약

- CNN은 완전 연결 신경망의 한계(공간적 정보 손실, 높은 복잡도)를 극복하기 위해 등장
- 컨볼루션 연산의 **지역적 연결**과 **가중치 공유**를 통해 효율적인 특징 추출
- Equivariance(등변성)와 풀링을 통한 Invariance(불변성) 특성으로 위치 변화에 강인한 모델 구현
- CNN의 발전 역사는 LeNet → AlexNet → ZFNet → VGGNet/GoogLeNet → ResNet으로 이어지며, 각 모델이 중요한 혁신을 제시
- 다양한 설계 요소(필터 크기, 스트라이드, 패딩, 풀링 등)와 기술(배치 정규화, 드롭아웃, 모델 경량화 등)을 활용하여 문제에 최적화된 CNN 구성 가능



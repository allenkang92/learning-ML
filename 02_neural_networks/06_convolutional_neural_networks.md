# 컨볼루션 신경망 (CNN)

## 1. 완전 연결 신경망(Fully-Connected Neural Network)의 한계와 CNN의 등장 배경

### 1.1 완전 연결 신경망의 한계

- **Fully-Connected (완전 연결) 특성**:
  - 이론적으로는 학습을 통해 어떤 문제든 해결 가능성을 가짐
  - **1차원 형태의 데이터 입력**을 요구함 → 이미지와 같은 다차원 데이터를 처리하기 위해서는 **Flatten 과정**이 필요

- **Flattening의 문제점**:
  - **공간적 정보(Locality) 손실**: 3차원 이미지를 1차원으로 변환하면 픽셀 간 인접 관계가 깨짐
  - **독립성 가정 위배**: 원래 데이터의 구조적 특성이 사라져 각 입력 요소가 독립적인 것처럼 처리됨
  - **차원의 저주(Curse of Dimensionality)**: 고차원 데이터를 1차원으로 펼치면서 입력 차원이 급증
  - 이러한 문제로 인해 **모델의 범용성(Generalization)이 저하**됨

### 1.2 과거의 해결 노력

- **강력한 전처리(Preprocessing)**:
  - 이미지 데이터를 모델이 잘 인식하도록 정중앙에 위치시키기
  - 크기를 일정하게 맞추기
  - 회전, 밝기 등 표준화

- **피처 엔지니어링(Feature Engineering)**:
  - 입력 데이터에서 중요 특징(Feature)을 수동으로 추출/가공
  - 어떤 특징을 어떻게 다뤄야 성능이 좋을지에 대한 연구에 많은 시간과 노력 투자
  - 도메인 지식에 크게 의존

- **데이터 증강(Data Augmentation)**:
  - 데이터가 중앙에 있지 않거나 다양한 변형이 있어도 모델이 강인하게 학습하도록 지원
  - 근본적인 해결책은 아니었음

## 2. 컨볼루션 신경망(CNN)의 핵심 아이디어

### 2.1 Invariance와 Equivariance

- **Invariance (불변성)**:
  - 입력의 위치가 변해도 출력은 변하지 않는 성질
  - 예: "Welcome"이라는 단어가 음성 신호의 어느 부분에 있든 "Welcome이 있다"는 동일한 출력
  - **Shift Invariance (이동 불변성)**: 패턴의 위치 변화에 영향받지 않는 특성

- **Equivariance (등변성/동변성)**:
  - 입력의 위치가 변하면 출력의 위치도 그에 맞춰 동일하게 변하는 성질
  - 예: 이미지에서 고양이 위치가 오른쪽으로 이동하면, 특징 맵에서도 고양이 활성화 영역이 오른쪽으로 이동
  - **컨볼루션 연산은 본질적으로 Equivariant** - 필터가 이미지 전체를 순회하며 특징을 감지
  - CNN에서 **풀링(Pooling) 레이어**를 통해 부분적인 Invariance 획득

### 2.2 CNN의 "스캔(Scan)" 방식

- **특징 감지 메커니즘**:
  - 작은 윈도우(필터)를 이동시키면서 각 영역에 특정 패턴이 있는지 확인
  - 음성 신호에서 "Welcome" 단어 찾기: 작은 "Welcome-탐지기" 필터를 이동하며 감지
  - 이미지에서 "꽃" 찾기: 이미지 각 부분을 작은 필터로 스캔하며 "꽃" 특징 감지

- **특징 추출 과정**:
  - 입력 전체를 스캔하여 "있다/없다" 형태의 특징 맵(Feature Map) 생성
  - 이 특징 맵을 학습에 활용하여 모델의 유연성과 효율성 향상

## 3. 컨볼루션(Convolution) 연산의 이해

### 3.1 기본 개념

- **컨볼루션 연산**:
  - 원래 **신호 처리(Signal Processing)** 분야에서 사용되던 기법
  - 수학적으로 두 함수 중 하나를 **뒤집고(flip)** 이동하면서 중첩 영역의 내적(또는 적분) 계산
  - `scipy.signal.convolve(a, b)`: 두 신호 `a`와 `b` 사이의 컨볼루션 계산

- **상관관계(Correlation)와 컨볼루션(Convolution)의 차이**:
  - **상관관계 연산**: 커널을 뒤집지 않고 내적 연산 수행
  - **컨볼루션 연산**: 커널을 상하좌우로 뒤집은 후 상관관계 연산 수행
  - **주요 차이점**: 커널을 뒤집느냐의 여부

### 3.2 컨볼루션 연산의 특성

- **왜 컨볼루션을 사용하는가?**:
  - **교환 법칙(Commutative Property) 성립**: $A * B = B * A$ (여기서 $*$는 컨볼루션 연산)
  - 교환 법칙이 성립하면 이론 정립과 수학적 처리가 용이
  - **딥러닝에서의 구현**: 대부분의 딥러닝 라이브러리는 실제로는 **상관관계 연산을 "컨볼루션"이라고 명명**
  - 커널 가중치가 학습을 통해 최적화되므로 초기에 뒤집혀 있든 아니든 최종 결과에 큰 영향 없음

- **컨볼루션 연산의 의미**:
  - **내적 연산의 공간적 의미**: 주변 픽셀들의 속성을 종합하여 하나의 값으로 표현
  - 입력 데이터에서 **특정 패턴(특징)의 존재 여부를 감지**하는 역할

### 3.3 Zero Padding과 Stride

- **Zero Padding (제로 패딩)**:
  - 입력 데이터 가장자리에 0 값을 추가하는 기법
  - **목적**:
    - 출력 특징 맵의 크기를 입력과 동일하게 유지/조절
    - 입력 데이터 가장자리 픽셀들도 동등하게 참여 (정보 손실 방지)

- **Stride (스트라이드)**:
  - 컨볼루션 필터를 이동시키는 간격
  - Stride가 1이면 한 픽셀씩, 2이면 두 픽셀씩 이동
  - Stride 값이 클수록 출력 특징 맵의 크기는 작아짐 (다운샘플링 효과)

## 4. CNN의 구조적 특징과 장점

### 4.1 역사적 배경 및 구조적 특징

- **얀 르쿤(Yann LeCun)의 기여**:
  - 1989년 LeNet-1을 통해 컨볼루션 연산을 신경망에 효과적으로 결합
  - 현대 CNN 아키텍처의 기초를 확립

- **컨볼루션 연산의 선형성**:
  - $Wx+b$ 형태의 연산으로 기존 신경망 구조에 자연스럽게 통합 가능

- **실제 구현 시 병렬 처리**:
  - 이론적으로는 필터가 순차적으로 이동하며 연산
  - 실제 GPU 구현에서는 각 위치 연산을 독립적으로 분리하여 병렬 처리 (벡터화)
  - 이를 통해 계산 속도를 크게 향상

### 4.2 주요 개념 및 장점

- **Locally Connected & Shared Weights (지역적 연결 및 가중치 공유)**:
  - **지역적 연결**: 각 뉴런이 입력의 일부 지역(Receptive Field, 수용 영역)에만 연결
  - **가중치 공유**: 동일한 필터(커널)가 입력 전체를 순회하며 사용됨
  - **장점**:
    - 파라미터 수 대폭 감소 → 모델 복잡도 감소 및 과적합 방지
    - 학습 효율 향상
    - 위치 변화에도 동일 특징 감지 가능 (Shift Invariance에 기여)

- **CNN의 기본 가정**:
  - **Stationarity of Statistics (통계적 항상성)**: 이미지 내 서로 다른 부분의 통계적 특성이 유사
  - **Locality of Pixel Dependencies (픽셀 종속성의 지역성)**: 인접 픽셀 간 높은 연관성, 원거리 픽셀 간 낮은 연관성

- **Statistical Efficiency (통계적 효율성)**:
  - 가중치 공유와 지역적 연결을 통해 적은 파라미터로 효과적인 특징 추출
  - 동일 양의 학습 데이터로 더 많은 정보 학습 → 일반화 능력 향상

- **공간적 정보 보존**:
  - Flatten 없이 2D/3D 형태 그대로 처리
  - 입력의 공간적 구조를 유지하며 특징 학습

## 5. 풀링 (Pooling) 레이어

### 5.1 개념 및 목적

- **목적**:
  - 특징 맵의 **차원 축소**(Downsampling) → 계산량 감소 및 과적합 방지
  - 입력의 작은 변화(이동, 회전, 크기 변화 등)에 대한 **불변성(Invariance) 제공**
  - 수용 영역(Receptive Field) 확장 → 더 넓은 컨텍스트 고려

- **종류**:
  - **Max Pooling (최대 풀링)**: 특정 영역에서 최대값 선택 (가장 두드러진 특징 강조)
  - **Average Pooling (평균 풀링)**: 특정 영역 값들의 평균 사용

### 5.2 풀링에 대한 논쟁과 대안

- **풀링 레이어의 역할에 대한 논쟁**:
  - **얀 르쿤**: Max Pooling이 불변성을 보존하는 장점 강조
  - **제프리 힌튼**: Max Pooling은 잘못된 접근이며, "Invariance"보다 "Equivariance"가 중요하다고 주장
    (이러한 관점은 후에 캡슐 네트워크 개발로 이어짐)

- **풀링 없는 CNN (All Convolutional Net)**:
  - 최근 경향: 풀링 레이어 대신 Stride가 큰 컨볼루션 레이어로 차원 축소
  - VAE, GAN 등에서 자주 사용되는 접근법

## 6. CNN 아키텍처 설계 시 고려 사항

- **컨볼루션 필터 크기 (Kernel Size)**:
  - 일반적으로 3x3, 5x5, 7x7 등 사용
  - 작은 커널 여러 개를 쌓는 것이 큰 커널 하나보다 효율적인 경우가 많음
  - 특징의 크기와 복잡도에 맞게 선택

- **풀링 레이어 사용 여부 및 종류**:
  - Max Pooling: 두드러진 특징 보존
  - Average Pooling: 전반적인 특성 보존
  - 생략 여부: 일부 모델은 Strided Convolution으로 대체

- **스트라이드 (Stride)**:
  - 필터 이동 간격 설정
  - 큰 스트라이드: 차원 축소 효과
  - 작은 스트라이드: 세밀한 특징 포착

- **패딩 (Padding)**:
  - Valid Padding: 패딩 없음 → 출력 크기 감소
  - Same Padding: Zero Padding 사용 → 출력 크기 유지
  - 출력 크기 관계: 
    - 출력 크기 = (입력 크기 - 커널 크기 + 2*패딩) / 스트라이드 + 1

## 7. CNN의 계층적 특징 학습

### 7.1 계층적 특징 추출

- **컨볼루션 레이어의 중첩**:
  - CNN은 여러 컨볼루션 레이어를 순차적으로 쌓아 구성

- **계층별 특징 학습**:
  - **초기 레이어(얕은 층)**: 단순한 특징 감지 (선, 모서리, 색상 등)
  - **중간 레이어**: 질감, 패턴, 단순한 형태 감지
  - **후기 레이어(깊은 층)**: 복잡하고 추상적인 특징 감지 (객체의 부분, 객체 전체 등)
  - 이러한 계층적 특징 학습을 통해 복잡한 시각적 패턴을 효과적으로 인식

### 7.2 Global Average Pooling (GAP)

- **개념 및 용도**:
  - CNN의 마지막 컨볼루션 레이어 다음에 주로 사용
  - 각 특징 맵의 모든 픽셀 값의 평균을 계산하여 하나의 값으로 변환

- **장점**:
  - 완전 연결 계층(FC Layer) 대체 → 파라미터 수 감소 및 과적합 방지
  - **Regularizer 역할** 수행
  - 입력 이미지의 공간적 변환에 대해 더 강인한(Robust) 특성
  - 내부 동작 해석이 FC 레이어보다 용이

## 8. CNN의 한계 및 발전 방향

### 8.1 기존 CNN의 한계

- **Hinton의 CNN 비판**: "What is wrong with convolutional neural nets?"
  - 객체의 상대적 공간 관계나 자세 변화 등을 제대로 처리하지 못하는 한계
  - 공간적 계층 구조(Spatial Hierarchy) 모델링의 한계
  - 회전, 크기 등 다양한 변환에 대한 진정한 불변성 부재

### 8.2 발전 방향 및 연구 동향

- **캡슐 네트워크(Capsule Network)**:
  - 객체의 부분들 간의 위치 관계를 보존하는 새로운 접근법
  - Dynamic Routing 메커니즘을 통한 특징 통합

- **어텐션 메커니즘(Attention Mechanism)**:
  - 이미지의 중요 부분에 집중하는 메커니즘 도입
  - Transformer 기반 컴퓨터 비전 모델로 확장 (Vision Transformer 등)

- **CNN 시각화 및 해석**:
  - 모델의 각 레이어가 학습하는 특징 시각화 기법 연구
  - Grad-CAM, Feature Visualization 등의 방법론
  - 모델의 내부 동작 이해 및 개선에 활용

## 9. 유명한 CNN 아키텍처

- **LeNet-5 (1998)**: 얀 르쿤이 개발한 최초의 성공적인 CNN 구조, 우편번호 인식에 사용
- **AlexNet (2012)**: ImageNet 대회에서 획기적인 성능으로 딥러닝 혁명 촉발
- **VGG (2014)**: 간단하고 균일한 구조로 3x3 컨볼루션 필터만 사용
- **GoogLeNet/Inception (2014)**: Inception 모듈 도입, 효율적인 네트워크 설계
- **ResNet (2015)**: 잔차 연결(Residual Connection)을 통한 깊은 네트워크 학습 가능화
- **DenseNet (2017)**: 모든 레이어를 서로 연결하여 특징 재사용 극대화
- **EfficientNet (2019)**: 네트워크 깊이, 너비, 해상도의 균형있는 확장 방법론 제시

## 핵심 요약

- CNN은 완전 연결 신경망의 한계(공간적 정보 손실, 높은 복잡도)를 극복하기 위해 등장
- 컨볼루션 연산의 **지역적 연결**과 **가중치 공유**를 통해 효율적인 특징 추출
- Equivariance(등변성)와 풀링을 통한 Invariance(불변성) 특성으로 위치 변화에 강인한 모델 구현
- 계층적 구조를 통해 단순한 특징부터 복잡한 특징까지 단계적으로 학습
- 다양한 설계 요소(필터 크기, 스트라이드, 패딩, 풀링 등)를 조합하여 문제에 맞는 최적의 아키텍처 구성 가능
- 기존 CNN의 한계를 극복하기 위한 다양한 발전 방향(캡슐 네트워크, 어텐션 메커니즘 등) 연구 진행 중



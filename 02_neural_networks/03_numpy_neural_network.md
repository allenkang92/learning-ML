# Numpy를 이용한 신경망 기초 구현

## 1. 신경망의 기초적 구현 의의

Numpy만을 사용하여 신경망을 구현하는 것은 딥러닝의 내부 작동 원리를 이해하는 데 매우 유용합니다. TensorFlow나 PyTorch와 같은 고수준 라이브러리는 신경망 구현을 간소화하지만, 기본 개념의 이해를 위해서는 저수준 구현 경험이 중요합니다.

## 2. 활성화 함수와 그 미분

신경망에서 활성화 함수는 비선형성을 도입하여 복잡한 패턴을 학습할 수 있게 합니다.

### 2.1 시그모이드(로지스틱) 함수
- **정의:** `σ(x) = 1/(1+e^(-x))`
- **범위:** (0, 1)
- **용도:** 이진 분류 문제의 출력층
- **미분:** `σ'(x) = σ(x) * (1 - σ(x))`
- **특징:** 그래디언트 값의 최대치는 0.25로, 깊은 네트워크에서 그래디언트 소실 문제를 야기할 수 있음

### 2.2 하이퍼볼릭 탄젠트(Tanh) 함수
- **정의:** `tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))` 또는 `tanh(x) = 2*σ(2x) - 1`
- **범위:** (-1, 1)
- **용도:** 은닉층 활성화 함수
- **미분:** `tanh'(x) = 1 - tanh^2(x)`
- **특징:** 출력이 평균적으로 0에 가까워(Zero-centered) 다음 층 학습에 유리

### 2.3 소프트맥스 함수
- **정의:** `softmax(x_i) = e^(x_i) / Σ_j e^(x_j)`
- **범위:** 각 요소는 (0, 1), 모든 요소의 합은 1
- **용도:** 다중 클래스 분류 문제의 출력층
- **특징:** 수치적 안정성을 위해 입력에서 최댓값을 빼는 기법 사용

## 3. 다중 작업 학습 (Multi-task Learning)

다중 작업 학습은 하나의 모델이 여러 관련된 작업을 동시에 학습하는 방법입니다.

### 3.1 장점
- 데이터 효율성 증가
- 특징 표현의 일반화 향상
- 작업 간 정보 공유로 성능 향상 가능

### 3.2 구현 방법
- 여러 작업에 대한 출력을 하나의 모델에서 생성
- 각 작업의 특성에 맞는 활성화 함수와 손실 함수 선택
- 전체 손실은 개별 작업 손실의 가중 합으로 계산

## 4. 손실 함수

각 작업의 특성에 맞는 손실 함수를 선택하는 것이 중요합니다.

### 4.1 제곱 오차 (Squared Error)
- **정의:** `SE(y, ŷ) = (y - ŷ)^2`
- **용도:** 회귀 문제

### 4.2 이진 교차 엔트로피 (Binary Cross-Entropy)
- **정의:** `BCE(y, ŷ) = -(y*log(ŷ) + (1-y)*log(1-ŷ))`
- **용도:** 이진 분류 문제

### 4.3 범주형 교차 엔트로피 (Categorical Cross-Entropy)
- **정의:** `CE(y, ŷ) = -Σ(y*log(ŷ))`
- **용도:** 다중 클래스 분류 문제

## 5. 신경망 학습 과정

### 5.1 순전파 (Forward Propagation)
1. 입력 데이터와 가중치, 편향을 사용해 선형 결합 계산: `Z = X @ W + B`
2. 각 작업에 맞는 활성화 함수 적용: `Y_pred = activation(Z)`

### 5.2 손실 계산
각 작업별로 적절한 손실 함수를 사용해 오차 계산

### 5.3 역전파 (Backpropagation)
1. 활성화 함수의 출력에 대한 손실의 그래디언트 계산
2. 체인 룰을 적용하여 가중치와 편향에 대한 그래디언트 계산
3. 경사 하강법을 사용해 파라미터 업데이트: `W -= lr * dL/dW`, `B -= lr * dL/dB`

### 5.4 단순화된 역전파
기본 개념 이해를 위한 간소화된 접근:
- `dL/dY_pred = Y_pred - Y` (손실 함수와 활성화 함수 미분의 조합)
- `W = W - lr * X.T @ dL/dY_pred`
- `B = B - lr * Σ(dL/dY_pred, axis=0)`

## 6. 코드 구현

실제 구현은 `multi_task_learning.py` 파일에서 확인할 수 있습니다. 
주요 구성 요소:
1. 라이브러리 임포트 및 기본 함수 정의
2. 학습 데이터 설정
3. 모델 초기화 (가중치, 편향)
4. 학습 루프 (순전파, 손실 계산, 역전파)
5. 학습 곡선 시각화
6. 학습된 모델로 예측 수행

## 7. 다층 퍼셉트론(MLP)과 XOR 문제 해결

### 7.1 XOR 문제와 단층 퍼셉트론의 한계

XOR(배타적 논리합) 연산은 입력값이 서로 다를 때만 1을 출력하는 논리 연산입니다:
- 0 XOR 0 = 0
- 0 XOR 1 = 1
- 1 XOR 0 = 1
- 1 XOR 1 = 0

이 문제는 단층 퍼셉트론으로는 해결할 수 없습니다. 그 이유는 XOR 연산의 결과가 선형적으로 분리 불가능한 데이터 구조를 가지기 때문입니다. 다시 말해, 2차원 평면에서 직선 하나로 두 클래스(0과 1)를 분리할 수 없습니다.

### 7.2 다층 퍼셉트론(MLP)의 구조

다층 퍼셉트론은 입력층과 출력층 사이에 하나 이상의 **은닉층(Hidden Layer)**을 가진 신경망 구조입니다:

1. **입력층**: 데이터를 받아들이는 층 (XOR 문제의 경우 2개의 노드)
2. **은닉층**: 중간 계산을 수행하는 층 (XOR 문제 해결에는 최소 2개 이상의 노드가 필요)
3. **출력층**: 최종 예측을 출력하는 층 (XOR 문제의 경우 1개의 노드)

각 층의 노드는 이전 층의 모든 노드와 연결되며, 연결마다 가중치가 존재합니다. 또한 각 층에는 편향(Bias)이 추가됩니다.

### 7.3 순전파(Forward Propagation)

순전파는 입력에서 출력 방향으로 데이터를 전달하는 과정입니다:

1. **입력층 → 은닉층**:
   - 가중합 계산: `Z1 = X @ W1 + B1`
   - 활성화 함수 적용: `A1 = logistic(Z1)`

2. **은닉층 → 출력층**:
   - 가중합 계산: `Z2 = A1 @ W2 + B2`
   - 활성화 함수 적용: `A2 = logistic(Z2)` (최종 예측값)

### 7.4 역전파(Backpropagation)

역전파는 출력층에서 입력층 방향으로 오차를 전파하며 가중치를 업데이트하는 과정입니다:

1. **출력층 오차 계산**:
   - `dZ2 = A2 - Y` (예측값과 실제값의 차이)

2. **은닉층→출력층 가중치/편향 업데이트**:
   - `dW2 = A1.T @ dZ2`
   - `dB2 = np.sum(dZ2, axis=0, keepdims=True)`

3. **은닉층 오차 계산**:
   - `dA1 = dZ2 @ W2.T`
   - `dZ1 = dA1 * dlogistic2(A1)`

4. **입력층→은닉층 가중치/편향 업데이트**:
   - `dW1 = X.T @ dZ1`
   - `dB1 = np.sum(dZ1, axis=0, keepdims=True)`

5. **가중치와 편향 갱신**:
   - `W2 -= lr * dW2`
   - `B2 -= lr * dB2`
   - `W1 -= lr * dW1`
   - `B1 -= lr * dB1`

### 7.5 XOR 문제 해결 원리

MLP가 XOR 문제를 해결할 수 있는 이유는 은닉층이 입력 공간을 새로운 특성 공간으로 변환(임베딩)하기 때문입니다. 이 변환된 공간에서는 데이터가 선형적으로 분리 가능해집니다. 

예를 들어, 은닉층이 2개의 노드를 가진다면:
1. 첫 번째 노드는 `(1,1)`에 가까울 때 활성화
2. 두 번째 노드는 `(0,0)`에 가까울 때 활성화

이렇게 변환된 특성 공간에서는 출력층이 선형 분리를 통해 XOR 연산 결과를 정확히 예측할 수 있게 됩니다.

### 7.6 구현 예제

`mlp_xor_gate.py` 파일에서 Numpy를 사용한 MLP 구현과 XOR 문제 해결 과정을 확인할 수 있습니다. 이 구현은 다음을 포함합니다:

1. 데이터셋 준비 (XOR 입력/출력)
2. 활성화 함수와 도함수 정의
3. 은닉층을 포함한 신경망 구조 설계
4. 순전파와 역전파를 포함한 학습 루프
5. 결과 시각화 (학습 곡선 및 결정 경계)

## 8. 결론

Numpy를 활용한 신경망 구현은 다음과 같은 의의를 갖습니다:
- 딥러닝의 기본 원리와 수학적 개념 이해
- 모델 작동 방식에 대한 직관 형성
- 프레임워크에 의존하지 않는 유연한 사고 개발
- 다양한 활성화 함수와 손실 함수의 특성 및 용도 학습
- 특히 단층에서 다층 구조로의 전환을 통해 비선형 문제 해결 능력 획득

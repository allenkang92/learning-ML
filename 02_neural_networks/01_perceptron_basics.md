# 인공신경망(ANN) 기초 및 퍼셉트론

## 1. 퍼셉트론(Perceptron)의 기본 개념

- **구성 요소:**
    - **입력 (Inputs - X):** 외부에서 받아들이는 신호 또는 데이터 값 (x1, x2, ...)
    - **가중치 (Weights - W):** 각 입력 신호의 중요도를 나타내는 값 (w1, w2, ...)
    - **가중합 (Weighted Sum) / 선형합 (Linear Sum) / Affine:** 입력과 가중치의 곱들을 모두 더한 값
    - **편향 (Bias - B):** 뉴런(노드)이 얼마나 쉽게 활성화될지를 조절하는 값
        - 수식: `Affine = (x1*w1 + ... + xn*wn) + b = X @ W.T + B`
    - **활성화 함수 (Activation Function):** 가중합+편향 값을 입력받아 최종 출력 신호를 결정하는 함수
    - **출력 (Output - Y):** 활성화 함수를 통과한 최종 결과 값

## 2. 인공신경망(ANN)의 발전 과정

- **ANN (Artificial Neural Network):** 생물학적 신경망에서 영감을 받아 만들어진 계산 모델
- **SLP (Single Layer Perceptron - 단층 퍼셉트론):**
    - 입력층과 출력층만으로 구성
    - **한계:** 선형적으로 분리 가능한 문제만 해결 가능, **XOR 문제** 해결 불가
- **MLP (Multi Layer Perceptron - 다층 퍼셉트론):**
    - 입력층과 출력층 사이에 하나 이상의 **은닉층(Hidden Layer)** 추가
    - XOR 문제 해결 가능
- **DNN (Deep Neural Network - 심층 신경망):**
    - **많은** 은닉층을 가진 MLP로 복잡한 패턴 학습 가능
    - **발전 배경:** ReLU 활성화 함수, 하드웨어 발전, 개선된 학습 기법 등

## 3. 신경망의 작동 원리 (계산 과정)

- **선형 변환:** `Affine = X @ W.T + B`
- **활성화 함수:**
    - **Sigmoid:** 출력을 0과 1 사이 값으로 변환
    - **Softmax:** 출력을 확률 분포로 변환
    - **ReLU:** 입력이 양수면 그대로, 음수면 0 출력
    - **Step Function:** 임계값 기준 0 또는 1 출력
- **순전파:** 입력층에서 출력층까지 신호가 전달되며 예측값을 계산하는 과정

## 4. 신경망 학습 과정

- **목표:** 예측값과 실제 정답 간의 오차를 최소화하도록 가중치와 편향을 조정
- **손실 함수:** 오차를 측정하는 함수 (MSE, Cross-Entropy 등)
- **경사 하강법:** 손실 함수의 기울기를 계산하여 기울기가 낮아지는 방향으로 가중치 업데이트
    - **업데이트 규칙:** `W_new = W_old - learning_rate * (dL/dW)`
- **역전파 & 연쇄 법칙:** 오차를 역방향으로 전파시키며 각 층의 가중치에 대한 기울기 계산
- **학습률:** 가중치 업데이트 크기 조절하는 하이퍼파라미터
- **배치:** 데이터 처리 방식 (Full-batch, Stochastic, Mini-batch)
- **옵티마이저:** 경사 하강법을 개선한 알고리즘 (Momentum, Adam, RMSprop 등)
- **에포크:** 전체 데이터셋을 한 번 학습한 횟수

## 5. AND 게이트 구현 개념

1. **데이터셋 준비:** AND 연산의 입력(X)과 정답(Y) 정의
2. **모델 초기화:** 입력 2개, 출력 1개인 단층 퍼셉트론, 가중치(W)와 편향(B) 랜덤 초기화
3. **학습 과정:**
   - 순전파: 입력값으로 예측 계산 `pred_y = x @ W.T + B`
   - 손실 계산: 제곱 오차(SE) 사용 `loss = ((y - pred_y) ** 2).sum() / 2`
   - 역전파: 손실에 대한 가중치/편향의 기울기 계산
   - 가중치 갱신: 기울기와 학습률을 사용하여 가중치와 편향 업데이트
4. **모델 평가:** 학습된 가중치로 새로운 입력에 대한 AND 연산 결과 예측
5. **실제 구현은 `perceptron.py` 파일에서 확인 가능**

## 6. 로지스틱 회귀 (단층 퍼셉트론)를 이용한 AND 게이트 구현

로지스틱 회귀는 시그모이드 활성화 함수를 사용하는 단층 퍼셉트론과 동일한 모델입니다.

### 6.1 모델 구성 요소

1. **입력 및 출력:**
   - **입력 (X):** AND 게이트 입력값 `[[0,0], [0,1], [1,0], [1,1]]`
   - **정답 (Y):** AND 연산 결과 `[0, 0, 0, 1]`

2. **가중치와 편향:**
   - **가중치 (W):** 각 입력 특성의 중요도 (랜덤 초기화)
   - **편향 처리:** 입력 데이터에 1을 추가한 열을 붙여서 가중치에 통합 처리
     ```python
     D = np.c_[np.ones(len(X)), X]  # 입력에 1 추가
     W = np.random.rand(3)  # 편향 + 입력 2개 = 3개 가중치
     ```

3. **활성화 함수:**
   - **시그모이드/로지스틱 함수:** 선형합의 결과를 0과 1 사이의 확률로 변환
     ```python
     logistic = lambda x: 1/(1+np.exp(-x))
     ```

4. **손실 함수:**
   - **이진 크로스 엔트로피 (BCE):** 이진 분류에 적합한 손실 함수
     ```python
     BCE = lambda y, predy: -y*np.log(predy)-(1-y)*np.log(1-predy)
     ```

### 6.2 학습 과정

1. **순전파:**
   - 가중합 계산: `ls = D @ W` (입력과 가중치의 내적 + 편향)
   - 예측값 계산: `predY = logistic(ls)` (활성화 함수 적용)

2. **역전파:**
   - 기울기 계산: `dLossY = -Y + predY` (BCE 손실과 시그모이드 미분의 결합된 형태)
   - 가중치 업데이트: `W = W - lr * (dW @ dLossY)` (경사 하강법)

3. **경사 하강법 특징:**
   - **풀배치 방식:** 전체 데이터에 대한 기울기를 한 번에 계산하여 적용
   - **학습률 (lr):** 가중치 업데이트 크기 조절 (여기서는 1e-4 사용)

### 6.3 모델 평가

1. 학습 후 예측: `predictions = logistic(D @ W)`
2. 임계값(0.5) 적용: `predictions > 0.5`로 이진 분류
3. 학습 곡선을 통해 손실 감소 추세 확인

### 6.4 로지스틱 회귀 vs. 순수 퍼셉트론

- **활성화 함수 차이:**
  - 로지스틱 회귀: 시그모이드 함수 (연속적인 확률 출력)
  - 순수 퍼셉트론: 계단 함수 (이진 출력)
  
- **손실 함수 차이:**
  - 로지스틱 회귀: 이진 크로스 엔트로피 (확률 기반)
  - 순수 퍼셉트론: 절대 오차 등 (이진 결과 기반)

- **실제 구현은 `logistic_and_gate.py` 파일에서 확인 가능**
